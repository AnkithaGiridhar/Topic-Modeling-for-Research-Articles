{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\n\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport matplotlib.pyplot as plt","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/janatahack-independence-day-2020-ml-hackathon/train.csv')\ntest = pd.read_csv('../input/janatahack-independence-day-2020-ml-hackathon/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['ALL'] = test['TITLE'] + test['ABSTRACT']\ntrain['ALL'] = train['TITLE'] + train['ABSTRACT']","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_ignore = ['ID','TITLE','ABSTRACT']\ncols_y = ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']\ncols_X = list(set(train.columns) - set(cols_ignore) - set(cols_y))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_labels = train[cols_y]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\nsentences = list(train[\"ABSTRACT\"])\nfor sen in sentences:\n    X.append(preprocess_text(sen))\n#sentences2 = list(train[\"TITLE\"])\n#X2 = []\n#for sen in sentences2:\n#    X2.append(preprocess_text(sen))\n#print(X1,X2)\n#X = train[['TITLE','ABSTRACT']]\ny = topic_labels.values","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 200\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\n\nglove_file = open('../input/glove6b200d/glove.6B.200d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\nembedding_matrix = zeros((vocab_size, 200))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":12,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ed077a2d9a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membeddings_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_inputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)(deep_inputs)\nLSTM_Layer_1 = LSTM(128)(embedding_layer)\ndense_layer_1 = Dense(6, activation='sigmoid')(LSTM_Layer_1)\n#LSTM_Layer_2 = Dense(50)(dense_layer_1)\n#dense_layer_2 = Dense(6, activation='relu')(LSTM_Layer_2)\nmodel = Model(inputs=deep_inputs, outputs=dense_layer_1)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_val, y_val, verbose=1)\n\nprint(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test['ALL']\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_2 = np.where(y_test>0.3,1,0)\ny_test_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame(y_test_2)\nsub['ID'] = test['ID']\nsub = sub.set_index('ID')\nsub.columns = cols_y\nsub = sub.to_csv('sub_av.csv')\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}